{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_itl import model, sampler, cost, kernel, estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set paths and load model config / ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./LS_Experiments/KDEF_single/KDEF_NE_itl_model_split1_CF/model/itl_model_config_20210130-101821.json ./LS_Experiments/KDEF_single/KDEF_NE_itl_model_split1_CF/model/itl_model_ckpt_20210130-101821.pt\n"
     ]
    }
   ],
   "source": [
    "# Set trained model paths\n",
    "base_experiment_path = './LS_Experiments/KDEF_single'\n",
    "model_name = 'KDEF_NE_itl_model_split1_CF'\n",
    "\n",
    "# get model config and ckpt\n",
    "base_model_path = os.path.join(base_experiment_path, model_name, 'model/')\n",
    "for fname in os.listdir(base_model_path):\n",
    "    if ('config' in fname) and (fname.split('.')[-1] == 'json'):\n",
    "        model_config_path = os.path.join(base_model_path, fname)\n",
    "    elif ('ckpt' in fname) and (fname.split('.')[-1] == 'pt'):\n",
    "        model_ckpt_path = os.path.join(base_model_path, fname)\n",
    "    else:\n",
    "        print(fname, 'does not exist')\n",
    "print(model_config_path, model_ckpt_path)\n",
    "\n",
    "# load ckpt and json\n",
    "with open(model_config_path, 'r') as f:\n",
    "    model_config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "data dimensions 126 7 136 NE\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------\n",
    "# Reading input/output data\n",
    "# ----------------------------------\n",
    "dataset = model_config['Data']['dataset']  \n",
    "theta_type = model_config['Data']['theta_type']\n",
    "inp_emotion = model_config['Data']['inpu_emotion'] \n",
    "inc_neutral = model_config['Data']['include_emotion']  \n",
    "use_facealigner = True if model_config['Data']['input_data_version'] == 'facealigner' else False\n",
    "\n",
    "data_path = './datasets/' + dataset + '_Aligned/' + dataset +'_LANDMARKS'  # set data path\n",
    "if dataset == 'Rafd':\n",
    "    # dirty hack only used to get Rafd speaker ids, not continuously ordered\n",
    "    data_csv_path = '/home/mlpboon/Downloads/Rafd/Rafd.csv'\n",
    "\n",
    "print('Reading data')\n",
    "if use_facealigner:\n",
    "    if dataset == 'KDEF':\n",
    "        from datasets.datasets import kdef_landmarks_facealigner\n",
    "        x_train, y_train, x_test, y_test, train_list, test_list = \\\n",
    "            kdef_landmarks_facealigner(data_path, inp_emotion=inp_emotion, inc_emotion=inc_neutral)\n",
    "    elif dataset == 'Rafd':\n",
    "        from datasets.datasets import rafd_landmarks_facealigner\n",
    "        x_train, y_train, x_test, y_test, train_list, test_list = \\\n",
    "            rafd_landmarks_facealigner(data_path, data_csv_path, inc_emotion=inc_neutral)\n",
    "else:\n",
    "    from datasets.datasets import import_kdef_landmark_synthesis\n",
    "    x_train, y_train, x_test, y_test = import_kdef_landmark_synthesis(dtype=input_data_version)\n",
    "\n",
    "n = x_train.shape[0]\n",
    "m = y_train.shape[1]\n",
    "nf = y_train.shape[2]\n",
    "print('data dimensions', n, m, nf, inp_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set ITL model\n",
    "assert model_config['Kernels']['kernel_input_learnable'] == False\n",
    "kernel_input = kernel.Gaussian(model_config['Kernels']['gamma_inp'])\n",
    "kernel_output = kernel.Gaussian(model_config['Kernels']['gamma_out'])\n",
    "kernel_freq = np.eye(nf) # can be added to ckpt or manually set as np.load(kernel_file)\n",
    "\n",
    "# define emotion sampler - this can also be added to ckpt\n",
    "if model_config['Data']['theta_type'] == 'aff':\n",
    "    from datasets.datasets import import_affectnet_va_embedding\n",
    "    affect_net_csv_path = ''  # to be set if theta_type == 'aff'\n",
    "    aff_emo_dict = import_affectnet_va_embedding(affect_net_csv_path)\n",
    "    \n",
    "    if dataset == 'KDEF':\n",
    "        aff_emo_match = {'NE': 'Neutral',\n",
    "                         'HA': 'Happy',\n",
    "                         'SA': 'Sad',\n",
    "                         'SU': 'Surprise',\n",
    "                         'AF': 'Fear',\n",
    "                         'DI': 'Disgust',\n",
    "                         'AN': 'Anger',\n",
    "                         }\n",
    "    elif dataset == 'Rafd':\n",
    "        aff_emo_match = {'neutral': 'Neutral',\n",
    "                         'happy': 'Happy',\n",
    "                         'sad': 'Sad',\n",
    "                         'surprised': 'Surprise',\n",
    "                         'fearful': 'Fear',\n",
    "                         'disgusted': 'Disgust',\n",
    "                         'angry': 'Anger',\n",
    "                         'contemptous': 'Contempt'\n",
    "                         }    \n",
    "    \n",
    "    \n",
    "    sampler_ = sampler.CircularSampler(data=dataset+theta_type,\n",
    "                                       inp_emotion=aff_emo_match[inp_emotion],\n",
    "                                       inc_emotion=inc_neutral,\n",
    "                                       sample_dict=aff_emo_dict)\n",
    "elif theta_type == '':\n",
    "    sampler_ = sampler.CircularSampler(data=dataset,\n",
    "                                       inc_neutral=inc_neutral)\n",
    "sampler_.m = m\n",
    "\n",
    "itl_model = model.SpeechSynthesisKernelModel(kernel_input, kernel_output,\n",
    "                                             kernel_freq=torch.from_numpy(kernel_freq).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(model_ckpt_path)\n",
    "itl_model.test_mode(x_train=x_train, thetas=sampler_.sample(m), alpha=ckpt['itl_alpha'])\n",
    "pred_test = itl_model.forward(x_test, sampler_.sample(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cost\n",
    "cost_pred = cost.speech_synth_loss(y_test, pred_test, sampler_.sample(m))\n",
    "print('cost test:', cost_pred)\n",
    "\n",
    "# compute expected euclidean distance between samples and mean for each emotion\n",
    "var_gt_em = 0\n",
    "var_test_em = 0\n",
    "var_gt = 0\n",
    "var_test = 0\n",
    "for i in range(m):\n",
    "    var_gt_em = np.sum(np.var(y_test[:,i,:].numpy(), axis=0))\n",
    "    var_test_em = np.sum(np.var(pred_test[:,i,:].numpy(), axis=0))\n",
    "    var_gt += var_gt_em\n",
    "    var_test += var_test_em\n",
    "    print('{:d}, {:.3f}, {:.3f}'.format(i, var_gt_em, var_test_em))\n",
    "print('{:.3f}, {:.3f}'.format(var_gt/m, var_test/m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xv2.diagonal(), xv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_output = pred_test*128\n",
    "check_output[0,0].reshape(68,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt_x = x_test[0].numpy().reshape(68, 2)\n",
    "plt_xt = pred_test[1, 4].detach().numpy().reshape(68, 2)\n",
    "if use_facealigner:\n",
    "    plt_x = plt_x * 128\n",
    "    plt_xt = plt_xt * 128\n",
    "plt_uv = plt_xt - plt_x\n",
    "plt.quiver(plt_x[:, 0], plt_x[:, 1], plt_uv[:, 0], plt_uv[:, 1], angles='xy')\n",
    "ax = plt.gca()\n",
    "ax.invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circular_sampling(theta1, theta2, num_samples):\n",
    "    angle1 = np.arctan2(theta1[1], theta1[0])\n",
    "    angle2 = np.arctan2(theta2[1], theta2[0])\n",
    "    angle1 = angle1 if angle1>=0 else angle1+(2*np.pi)\n",
    "    angle2 = angle2 if angle2>=0 else angle2+(2*np.pi)\n",
    "    \n",
    "    reverse = False\n",
    "    if angle1>angle2:\n",
    "        start = angle2; end = angle1\n",
    "        reverse = True\n",
    "    else:\n",
    "        start = angle1; end = angle2\n",
    "        \n",
    "    sampled_angles = np.linspace(start=start, stop=end, num=num_samples, endpoint=True)\n",
    "    sample_coords = np.vstack((np.cos(sampled_angles), np.sin(sampled_angles))).T\n",
    "    \n",
    "    if reverse:\n",
    "        return np.flipud(sample_coords)\n",
    "    else:\n",
    "        return sample_coords, sampled_angles\n",
    "\n",
    "def radial_sampling(theta, num_samples):\n",
    "    angle = np.arctan2(theta[1], theta[0])\n",
    "    sampled_radii = np.linspace(start=0, stop=1, num=num_samples, endpoint=True)\n",
    "    sample_coords = np.vstack((sampled_radii*np.cos(angle), sampled_radii*np.sin(angle))).T\n",
    "    return sample_coords, sampled_radii\n",
    "\n",
    "\n",
    "class EdgeMap(object):\n",
    "    def __init__(self, out_res, num_parts=3):\n",
    "        self.out_res = out_res\n",
    "        self.num_parts = num_parts\n",
    "        self.groups = [\n",
    "            [np.arange(0, 17, 1), 255],\n",
    "            [np.arange(17, 22, 1), 255],\n",
    "            [np.arange(22, 27, 1), 255],\n",
    "            [np.arange(27, 31, 1), 255],\n",
    "            [np.arange(31, 36, 1), 255],\n",
    "            [list(np.arange(36, 42, 1)) + [36], 255],\n",
    "            [list(np.arange(42, 48, 1)) + [42], 255],\n",
    "            [list(np.arange(48, 60, 1)) + [48], 255],\n",
    "            [list(np.arange(60, 68, 1)) + [60], 255]\n",
    "        ]\n",
    "\n",
    "    def __call__(self, shape):\n",
    "        image = np.zeros((self.out_res, self.out_res, self.num_parts), dtype=np.float32)\n",
    "        for g in self.groups:\n",
    "            for i in range(len(g[0]) - 1):\n",
    "                start = int(shape[g[0][i]][0]), int(shape[g[0][i]][1])\n",
    "                end = int(shape[g[0][i + 1]][0]), int(shape[g[0][i + 1]][1])\n",
    "                cv2.line(image, start, end, g[1], 1)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10528595 0.12439157 0.14349718 0.1626028  0.18170841 0.20081403\n",
      " 0.21991964 0.23902526 0.25813087 0.27723649 0.2963421  0.31544771\n",
      " 0.33455333 0.35365894 0.37276456 0.39187017 0.41097579 0.4300814\n",
      " 0.44918702 0.46829263 0.48739825 0.50650386 0.52560948 0.54471509\n",
      " 0.56382071 0.58292632 0.60203194 0.62113755 0.64024317 0.65934878\n",
      " 0.6784544  0.69756001 0.71666563 0.73577124 0.75487685 0.77398247\n",
      " 0.79308808 0.8121937  0.83129931 0.85040493 0.86951054 0.88861616\n",
      " 0.90772177 0.92682739 0.945933   0.96503862 0.98414423 1.00324985\n",
      " 1.02235546 1.04146108 1.06056669 1.07967231 1.09877792 1.11788354\n",
      " 1.13698915 1.15609477 1.17520038 1.194306   1.21341161 1.23251722\n",
      " 1.25162284 1.27072845 1.28983407 1.30893968 1.3280453  1.34715091\n",
      " 1.36625653 1.38536214 1.40446776 1.42357337 1.44267899 1.4617846\n",
      " 1.48089022 1.49999583 1.51910145 1.53820706 1.55731268 1.57641829\n",
      " 1.59552391 1.61462952 1.63373514 1.65284075 1.67194636 1.69105198\n",
      " 1.71015759 1.72926321 1.74836882 1.76747444 1.78658005 1.80568567\n",
      " 1.82479128 1.8438969  1.86300251 1.88210813 1.90121374 1.92031936\n",
      " 1.93942497 1.95853059 1.9776362  1.99674182 2.01584743 2.03495305\n",
      " 2.05405866 2.07316428 2.09226989 2.11137551 2.13048112 2.14958673\n",
      " 2.16869235 2.18779796 2.20690358 2.22600919 2.24511481 2.26422042\n",
      " 2.28332604 2.30243165 2.32153727 2.34064288 2.3597485  2.37885411\n",
      " 2.39795973 2.41706534 2.43617096 2.45527657 2.47438219 2.4934878\n",
      " 2.51259342 2.53169903 2.55080465 2.56991026 2.58901588 2.60812149\n",
      " 2.6272271  2.64633272 2.66543833 2.68454395 2.70364956 2.72275518\n",
      " 2.74186079 2.76096641 2.78007202 2.79917764 2.81828325 2.83738887\n",
      " 2.85649448 2.8756001  2.89470571 2.91381133 2.93291694 2.95202256\n",
      " 2.97112817 2.99023379 3.0093394  3.02844502 3.04755063 3.06665624\n",
      " 3.08576186 3.10486747 3.12397309 3.1430787  3.16218432 3.18128993\n",
      " 3.20039555 3.21950116 3.23860678 3.25771239 3.27681801 3.29592362\n",
      " 3.31502924 3.33413485 3.35324047 3.37234608 3.3914517  3.41055731\n",
      " 3.42966293 3.44876854 3.46787416 3.48697977 3.50608539 3.525191  ]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "sampling_type = 'circular'\n",
    "num_samples = 180\n",
    "ckpt = torch.load(model_ckpt_path)\n",
    "itl_model.test_mode(x_train=x_train, thetas=sampler_.sample(m), alpha=ckpt['itl_alpha'])\n",
    "if sampling_type == 'circular':\n",
    "    sampled_emotions, sampled_angles = circular_sampling(aff_emo_dict['Happy'], aff_emo_dict['Sad'], num_samples)\n",
    "    print(sampled_angles)\n",
    "elif sampling_type == 'radial':\n",
    "    sampled_emotions, sampled_radii = radial_sampling(aff_emo_dict['Fear'], num_samples)\n",
    "    print(sampled_radii)\n",
    "EM = EdgeMap(out_res=128, num_parts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# for i in range(len(sampled_emotions)):\n",
    "#     pred_test = itl_model.forward(x_test, torch.from_numpy(sampled_emotions[i][np.newaxis]).float())\n",
    "#     im_em = EM(pred_test[0, 0].detach().numpy().reshape(68,2)*128)\n",
    "#     plt.imshow(np.squeeze(im_em))\n",
    "#     plt.pause(0.5)\n",
    "    \n",
    "output_path_cont_gen = './utils/plot_utils/visualizations/continuous_control/circ_video_kdef/happy_sad'\n",
    "if not os.path.exists(output_path_cont_gen):\n",
    "    os.makedirs(output_path_cont_gen)\n",
    "for i in range(len(sampled_emotions)):\n",
    "    pred_test = itl_model.forward(x_test, torch.from_numpy(sampled_emotions[i][np.newaxis]).float())\n",
    "    im_em = EM(pred_test[0, 0].detach().numpy().reshape(68,2)*128)\n",
    "    cv2.imwrite(os.path.join(output_path_cont_gen, str(i).zfill(3)+'.jpg'), im_em)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behaviour of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "imlist = []\n",
    "for i in range(len(sampled_emotions)):\n",
    "    pred_test = itl_model.forward(x_test, torch.from_numpy(sampled_emotions[i][np.newaxis]).float())\n",
    "    im_em = EM(pred_test[0, 0].detach().numpy().reshape(68,2)*128)\n",
    "    imlist.append(transforms.ToTensor()(im_em.copy()))\n",
    "    #imlist.append(transforms.ToTensor()(im_em.copy()))\n",
    "#show(make_grid(imlist, nrow=10, padding=10, pad_value=1))\n",
    "#save_image(imlist, 'radial_happy_to_surprise.jpg', nrow=10, padding=10, pad_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if dataset == 'KDEF':\n",
    "    MODEL_PATH = ''\n",
    "elif dataset == 'Rafd':\n",
    "    num_classes = 8\n",
    "    MODEL_PATH = './utils/landmark_utils/Classification/LndExperiments/Rafd_bs16_e10_20201118-055249'\n",
    "\n",
    "# model def\n",
    "def model(model_name, num_classes):\n",
    "    if model_name == 'resnet-18':\n",
    "        model_ft = models.resnet18(pretrained=False)\n",
    "        model_ft.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n",
    "                                   bias=False)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model_ft\n",
    "\n",
    "\n",
    "# Get ResNet and load wts\n",
    "emo_model_ft = model('resnet-18', num_classes)\n",
    "emo_model_ft.load_state_dict(torch.load(MODEL_PATH, map_location=lambda storage, loc: storage))\n",
    "emo_model_ft = emo_model_ft.to(device)\n",
    "emo_model_ft.eval()\n",
    "\n",
    "inputs = F.interpolate(torch.stack(imlist), size=224, mode='bilinear')\n",
    "outputs = emo_model_ft(inputs/255.)\n",
    "sout = nn.functional.softmax(outputs, dim=1)\n",
    "sout_np = sout.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate combined continuous generation and classifier behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 'Rafd':\n",
    "    plt_emo_labels = ['angry', 'contempt', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
    "elif dataset == 'KDEF':\n",
    "    plt_emo_labels = ['AF', 'AN', 'DI', 'HA', 'NE', 'SA', 'SU']    \n",
    "rows = 2\n",
    "columns = 8\n",
    "fig=plt.figure(figsize=(32,8))\n",
    "gs = fig.add_gridspec(3, 8)\n",
    "for i in range(1, rows*columns + 1):\n",
    "    if i <=(rows*columns/2):\n",
    "        fig.add_subplot(gs[0:2, i-1])\n",
    "        plt.imshow(np.squeeze(imlist[i-1].numpy()), cmap='gray')\n",
    "        plt.yticks([])\n",
    "        plt.xticks([])\n",
    "    else:\n",
    "        fig.add_subplot(gs[2:, int(i-(rows*columns/2)-1)])\n",
    "        plt.imshow(sout.detach().numpy()[int(i-1-(rows*columns/2))][:,np.newaxis], cmap='Reds', interpolation='nearest')\n",
    "        if i == 1+(rows*columns/2):\n",
    "            plt.yticks(np.linspace(0, num_classes-1, endpoint=True, num=8), plt_emo_labels, fontsize=20)\n",
    "        else:\n",
    "            plt.yticks([])\n",
    "        plt.xticks([])\n",
    "plt.subplots_adjust(hspace = -0.1, wspace=0.05)\n",
    "plt.savefig('plot_cls_cont.png',bbox_inches='tight',pad_inches=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(inputs[-1].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test[0,0]*128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = F.resize(torch.stack(imlist), size=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = transforms.ToPILImage()(imlist[-1]/255.)\n",
    "inp = transforms.Resize(size=224)(inp)\n",
    "inp = transforms.ToTensor()(inp)\n",
    "inp = inp.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_inp = emo_model_ft(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "im = Image.open(\"./utils/landmark_utils/Classification/LndPredRafd_itl_model_20201118-134437/angry/pred_Rafd090_25_Caucasian_male_angry_frontal.JPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imlist[-1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inppil = transforms.Grayscale()(im)\n",
    "inppil = transforms.Resize(size=224)(inppil)\n",
    "inppil = transforms.ToTensor()(inppil)\n",
    "inppil = inppil.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outpil = emo_model_ft(inppil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(os.listdir(\"./utils/landmark_utils/Classification/LndPredRafd_itl_model_20201118-134437/angry/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config['Kernels']['gamma_inp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config['Kernels']['gamma_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
